#!/bin/bash 

###############################################################################
# Author: Shelby Sturgis
# Date: 10-08-2013
# Notes: This script creates new collections and workflows for
# Azkaban jobs. 
###############################################################################

###############################################################################
# COLLECTION
# A collection is a group of workflows.  All collections which are
# created with this script will have a .properties, .job, and a params.sh 
# shell script (which passes dynamic parameters to each job in the child 
# workflows). To create a collection directory, you run the following
# command at the command line:
#
# 	$ azkaban create collection <directory_name> 
#
# where <directory_name> is the name of the collection directory.
#
# GLOBAL PROPERTIES FILE
# The .properties file in the collection folder is a global properties
# file which passes its parameters to all jobs in the workflow directories.
# All .properties files have the following global parameters:
#
# 	mysql_db: name of the MySQL database
#	mysql_server: address and port of the MySQL server
#		e.g. mysql_server=crystaldb01.dev.nyc1.beats:3306
#	hdfs_db:  name of the HDFS database
#	mysql_user: MySQL username
#	mysql_pswd: MySQL password
#
# .JOB FILE
# The .job file in the collection folder is a Azkaban job file which
# runs a bash command. The .job file runs the params.sh shell script, 
# which passes the pre-defined dynamic variables to the workflow jobs. 
#
# PARAMS.SH FILE
# The params.sh file in the bash file writes dynamic date parameters to
# the JOB_OUTPUT_PROP_FILE within Azkaban. It is the only way in which
# we are able to pass dynamic parameters, i.e. parameters that change
# on every run. For more information, see
# http://azkaban.github.io/azkaban2/documents/2.1/jobconf.html
#
# FLOW
# A workflow is a collection of jobs which get executed.
# All workflow directories which are created with this script will have
# a flow.properties, hive.job, sqooop.job, and qa.job file. To create
# a work "flow" folder, you run the following command at the command
# line:
#
#	$ azkaban create flow <directory_name>
#
# where <directory_name> is the name of the workflow directory. 
#
# WORKFLOW DIRECTORIES ARE CREATED WITHIN A COLLECTION.
# ALL MySQL AND HDFS DATABASE PARAMETERS NEEDED FOR JOBS IN THE WORKFLOW
# DIRECTORY ARE INHERITED FROM THE GLOBAL PROPERTIES (CONTROLLER.PROPERTIES)
# FILE LOCATED IN THE COLLECTION DIRECTORY.
#
# FLOW.PROPERTIES
# The flow.properties file is a local properties file. Jobs within
# the workflow folder inherit parameters from both the global and local
# properties file. Local properties override global properties. 
# The flow.properties file have the following local parameters:
#
#	mysql_qa_table: the MySQL QA table
#	mysql_dest_table: the MySQL table to which HDFS data is sqooped
#	hdfs_table: the HDFS table to which data is inserted
#
# HIVE.JOB
# The hive.job file runs a HIVE query, which pulls data from the 
# Supereventsfact table and inserts them into the appropriate HDFS table.
# The HDFS table is defined in the flow.properties file as hdfs_table.
#
# SQOOP.JOB
# The sqoop.job file runs a sqoop command which transfers data in the
# HDFS table to the corresponding MySQL table. The HDFS and MySQL tables
# are defined in the flow.properties file as hdfs_table and 
# mysql_dest_table. 
#
# QA.JOB
# The qa.job file is currently empty. The MySQL QA table is defined in
# the flow.properties file as mysql_qa_table.
#
# These are simply suggestions for what files are needed within 
# collection and flow directories. New files and directories may be
# added and both collection and workflow directories can be customized 
# to fit your needs.
###############################################################################

# GLOBAL PARAMETERS.
###############################################################################
PARAM_1=$1 
PARAM_2=$2 
PARAM_3=$3 
CURRENT_PATH=$(pwd)
CURRENT_DIR=${PWD##*/}
AZKABAN_DIR=$CURRENT_PATH/$PARAM_3
AZKABAN_SERVER="https://hdfs01.alpha.nyc1.beats:8444"
###############################################################################

# FUNCTIONS.
###############################################################################
# Creates the collection directory.
create_collection_dir() {
	mkdir $AZKABAN_DIR

	return
}

# Creates the files that go into the collection directory.
# Files include controller.properties, controller.job, params.sh,
# final_<collection directory>.job, and the HQL and SQL schema file.
create_collection_files() {
	# Local variables.
	#######################################################################
	local email
	local final_job

	# The name of the final job in the collection directory.
	# It consists of 'final_' and the name of the collection directory.
	# This naming convention was chosen to signify that it is the final 
	# job for each particular collection, which makes it 
	# easier to lookup the job status on Azkaban. 
	final_job="final_$PARAM_3"

	# Prompts users for the email address(es) where job failure notices 
	# are sent.
	read -p "Enter email address for failure emails > " email
	
	#######################################################################

	# Collection directory files.
	#######################################################################
	# HQL and SQL schemas. These files are currently empty.
	touch $AZKABAN_DIR/hql_schemas.hql $AZKABAN_DIR/sql_schemas.sql

	# controller.properties => global properties file
	cat > $AZKABAN_DIR/controller.properties <<- _EOF_
		mysqldb=reporting
		mysql_server=crystaldb01.dev.nyc1.beats:3306
		hdfs_db=reporting_nyc
		hdfs_db_loc=/user/hive/warehouse/reporting_nyc.db/
		mysql_user=angel
		mysql_pswd=@ng3L_1
		mysql_qa_table=qa_etl_hdfs
		email=$email
		_EOF_

	# controller.job => parent (first job) that starts a collection 
	# run in Azkaban.
	cat > $AZKABAN_DIR/controller.job <<- _EOF_
		type=command
		command=sh params.sh
		command.1=echo "Starting collection run..."
		_EOF_

	# params.sh
	cat > $AZKABAN_DIR/params.sh <<- _EOF_
		#!/bin/bash

		###############################################################
		# This file is created to pass dynamic date time variables
		# between Azkaban jobs. You can set the date and time 
		# parameters using the date shell function. For example, to 
		# set today's date with the following output '2013-09-30', 
		# you'd write:
		# 	today=\$(date -u +"%F")
		# To set a timestamp, you'd write:
		#	finish_ts=\$(date -u +"%s")
		#
		# Do not forget to use the -u tag to set the time in UTC.
		#
		# By convention,'today'is set to the current date.
		# 'finish_ts' is set to the current timestamp. 
		# 'start_ts' is set to yesterday's timestamp.
		################################################################

		# Date parameters
		today=\$(date -u +"%F")
		finish_ts=\$(date +%s --date="\$(date -u +"%F")")
		start_ts=\$(date +%s --date="\$(date -u +"%F" --date "1 day ago")")

		# Writing date parameters to temporary properties file.
		# Passing properties in this way is only good for passing 
		# between two connected jobs and not for jobs further down 
		# the dependency chain.
		# e.g. if A -> B -> C, properties passed from A will only 
		# be visible in B.
		echo '{ "date_part": "'\$today'", 
			"start_ts": "'\$start_ts'", 
			"finish_ts": "'\$finish_ts'"}' > JOB_OUTPUT_PROP_FILE
		_EOF_

	# final_<collection_directory>.job
	cat > $AZKABAN_DIR/$final_job.job <<- _EOF_
		type=command
		dependencies=
		failure.emails=\${email}
		command=echo "Collection run complete."
		_EOF_
	
	# Not sure if a final_job.properties file is needed.
	# Myabe able to add the parameters to the final_job.job file.
	# cat > $final_job/$final_job.properties <<- _EOF_
	#	mysql_qa_table=qa_etl_hdfs
	#	failure.emails=
	#	_EOF_
	#######################################################################

	return
}

# Creates the flow directory.
create_flow_dir() {
	mkdir $AZKABAN_DIR

	return
}

# Creates the files that go into the flow directory.
# Files include local properties file, hive.job, sqoop.job, and qa.job.
create_flow_files() {
	# Local variables.
	#######################################################################	
	local db_table
	local hive
	local sqoop
	local qa
	local parent
	local child

	# Prompts user for parent(first) and child(final) jobs, which are
	# typically located in the collection directory unless flows are nested
	# within flows..
	read -p "Enter the name of the parent job [controller] > " parent
	read -p "Enter the name of the final job [final_$CURRENT_DIR] > " child

	# Sets the default values as controller and final_<collection_dir> 
	# if no values were set.
	parent=${parent:="controller"}
	child=${child:="final_$CURRENT_DIR"}

	# Name for the database table.
	# The convention is that the database table has the same name
	# as the flow directory.
	db_table=$PARAM_3

	# Naming convention for the jobs in the flow directory.
	hive=$PARAM_3"_hive"
	sqoop=$PARAM_3"_sqoop"
	qa=$PARAM_3"_qa"
	#######################################################################	
	 
	# Creates the files in the flow directory.
	#######################################################################	
	# Local properties file.
	cat > $AZKABAN_DIR/$db_table.properties <<- _EOF_
	mysql_dest_table=$db_table
	hdfs_table=$db_table
	_EOF_

	# hive.job
	cat > $AZKABAN_DIR/$hive.job <<- _EOF_
	type=command
	dependencies=$parent
	command=sh ../params.sh
	command.1=hive -e ' '
	_EOF_

	# sqoop.job
	cat > $AZKABAN_DIR/$sqoop.job <<- _EOF_
	type=command
	dependencies=$hive
	command=sh ../params.sh
	command.1=sqoop export --connect jdbc:mysql://\${mysql_server}/\${mysql_db} --username \${mysql_user} --password \${mysql_pswd} --export-dir \${hdfs_db_loc} \${hdfs_table}/date_part=\${date_part} --table \${mysql_dest_table} --input-fields-terminated-by '\t' --input-null-string "\\\\\\\\\\N"
	_EOF_

	# qa.job
	cat > $AZKABAN_DIR/$qa.job <<- _EOF_
	type=command
	dependencies=$sqoop
	command=
	_EOF_

	# Adds new flow dependencies to the final job file.
	# Assumes that the final job is one directory up.
	# This assumption is not good and needs to be changed.
	sed -i '' '2s/$/&'$qa',/' $child.job
	#######################################################################	

	# Completion echo statements.
	echo ""
	echo "Workflow directory '$AZKABAN_DIR' created."
	echo ""
	echo "***** DO NOT FORGET TO ADD $qa.job TO THE final_<collection>_job.job DEPENDENCIES. *****"
	echo ""

	return
}

# Removes the zip file if it already exists.
remove_zip_file() {
	echo "Removing $PARAM_3.zip..."
	rm $CURRENT_PATH/$PARAM_3.zip
	echo "Zipped file $PARAM_3.zip removed."

	return
}

# Zips directory. 
zip_file() {
	if [[ -d "$CURRENT_PATH/$PARAM_3" ]]; then
		# Removes zipped file if it already exists.
		if [[ -e "$CURRENT_PATH/$PARAM_3.zip" ]]; then
			remove_zip_file
		fi

		echo "Zipping $CURRENT_PATH/$PARAM_3 directory."
		zip -qr $CURRENT_PATH/$PARAM_3.zip $CURRENT_PATH/$PARAM_3
	else
		echo $'\e[31mERROR: The $CURRENT_PATH/$PARAM_3 directory does not exist.'
	fi

	return
}

# Returns the Azkaban session.id from a curl call.
get_session_id () {
	# Local variables.
	local response
	local status
	local session_id

	# Curl call to Azkaban that returns a session_id in json.
	response=$(curl -s -k --data "action=login&username=azkaban&password=azkaban" $AZKABAN_SERVER)

	# Returns the job status in json.
	status=$(echo $response | awk '{print $4}' | tr -d '",')

	if [[ $status != "success" ]]; then
		echo ""
		echo $'\e[31m''****** ERROR: Cannot return session_id ******'$'\e[0m'
		echo $response
		exit 1
	fi

	session_id=$(echo $response | awk '{print $7}' | tr -d '"')

	# Returns session.id
	echo $session_id
}

upload_file() {
	# Local variables.
	local project
	local response
	local session_id

	# Azkaban project folder. Should be the name of the collection folder.
	read -p "What is the Azkaban project directory? [$PARAM_3] > " project
	project=${project:=$PARAM_3}

	# Check to make sure collection is in the current working directory.
	if [[ -d $AZKABAN_DIR ]]; then
		# Need to add zipped file function with a zip file name.
		# Zipped file name is the 3rd parameter + .zip
		# Currently hard coded in the curl call.
		zip_file

		# Retreives the session_id
		echo "Retreiving session.id...."
		session_id=$(get_session_id)

		# Uploads collection folder to Azkaban.
		if [[ $session_id == *ERROR* ]]; then
			echo $session_id
			echo "Upload failed."
			exit 1
		else
			echo "session.id: $session_id"
			echo "Uploading file to Azkaban...."
			response=$(curl -s -k -i -H "Content-Type: multipart/mixed" -X POST --form 'session.id='$session_id'' --form 'ajax=upload' --form 'file=@'$PARAM_3'.zip;type=application/zip' --form 'project='$project'' ''$AZKABAN_SERVER'/manager')

			if [[ $response == *error* ]]; then
				echo "ERROR: Cannont upload file to Azkaban."
				echo $response
				remove_zip_file
				exit 1
			else
				echo "Upload to Azkaban project $project successful!"
				echo $response
				remove_zip_file
			fi
		fi
	else
		echo "The $PARAM_3 directory could not be found."
		exit 1
	fi

		return
}

execute_uploaded_file() {
	# Local variable.
	local email
	local flow
	local project
	local response
	local session_id
	local status

	# need to ask where failure emails should go, should default to team-analytics@beatsmusic.com
	# The final job in every flow
	email="ssturgis@beatsmusic.com"
	project=$PARAM_3
	flow="final_"$PARAM_3"_job"
	session_id=$(get_session_id)

	# Check to see if the collection is in the current directory.
	if [[ -d $AZKABAN_DIR ]]; then
		# run curl command that executes uploaded file
		response=$(curl -s -k -i -X GET "$AZKABAN_SERVER/executor?session.id=$session_id&ajax=executeFlow&project=$project&flow=$flow&failureAction=finishPossible&failureEmailsOverride=true&failureEmails=$email")

		if [[ $response == *error* ]]; then
			echo "ERROR: Job could not be executed."
			echo $response
			exit 1
		else
			echo "Job is executing...."
			echo $response
		fi
	else
		echo "The $PARAM_3 directory could not be found."
		exit 1
	fi

	return
}

# The main function.
main() {
	# Local variables.
	local parent
	local child

	if [[ $PARAM_1 = "create" ]]; then
		if [[ $PARAM_2 = "collection" ]]; then
			if [[ -z "$PARAM_3" || -d "$AZKABAN_DIR" ]]; then
				if [[ -z "$PARAM_3" ]]; then
					echo ""
					echo $'\e[31m''****** ERROR: There is a missing parameter after '$PARAM_2'. ******'
					echo 'Please choose a name for your collection.'$'\e[0m'
					echo ""
					exit 1
				elif [[ -d "$AZKABAN_DIR" ]]; then
					echo ""
					echo $'\e[31m''****** ERROR: The '$PARAM_3' directory already exists. ******'
					echo 'Please choose another name for your collection.'$'\e[0m'
					echo ""
					exit 1
				fi
			else
				create_collection_dir
				create_collection_files
				echo ""
				echo "Collection directory '$AZKABAN_DIR' created."
				echo ""
			fi
		elif [[ $PARAM_2 = "flow" ]]; then
			if [[ -z "$PARAM_3" || -d "$AZKABAN_DIR" ]]; then
				if [[ -z "$PARAM_3" ]]; then
					echo ""
					echo $'\e[31m''****** ERROR: There is a missing parameter after '$PARAM_2'. ******'
					echo 'Please choose a name for your workflow directory.'$'\e[0m'
					echo ""
					exit 1
				elif [[ -d "$AZKABAN_DIR" ]]; then
					echo ""
					echo $'\e[31m''****** ERROR: The '$PARAM_3' directory already exists. ******'
					echo 'Please choose another name for your workflow directory.'$'\e[0m'
					echo ""
					exit 1
				fi
			else
				create_flow_dir
				create_flow_files 
			fi
		else
			echo ""
			echo $'\e[31m''****** ERROR: '$PARAM_2' is not a known command. ******'$'\e[0m'
			echo $'\e[36m''The following commands are used after the '$PARAM_1' parameter:
			1) collection
			2) flow
			'$'\e[0m'
			echo ""
			exit 1
		fi
	elif [[ $PARAM_1 = "upload" ]]; then
		if [[ $PARAM_2 = "collection" ]]; then
			# uploads folder to Azkaban
			upload_file
		else
			echo ""
			echo $'\e[31m''****** ERROR: '$PARAM_2' is not a known command. ******'$'\e[0m'
			echo $'\e[36m''The following command is used after the '$PARAM_1' parameter:
			1) collection
			'$'\e[0m'
			echo ""
			exit 1
		fi
	elif [[ $PARAM_1 = "execute" ]]; then
		if [[ $PARAM_2 = "collection" ]]; then
			upload_file
			execute_uploaded_file
		else
			echo ""
			echo $'\e[31m''****** ERROR: '$PARAM_2' is not a known command. ******'$'\e[0m'
			echo $'\e[36m''The following command is used after the '$PARAM_1' parameter:
			1) collection
			'$'\e[0m'
			echo ""
			exit 1
		fi
	else
		echo ""
		echo $'\e[31m''****** ERROR: '$PARAM_1' is not a known command. ******' $'\e[0m' 
		echo $'\e[36m''The following parameters are used:
		1) create
		2) upload
		3) execute
		'$'\e[0m'
		echo ""

		exit 1
	fi

	return
}
###############################################################################

# MAIN FUNCTION
main
